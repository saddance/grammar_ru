{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.665428170Z",
     "start_time": "2023-10-22T12:36:00.833445255Z"
    }
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "import seaborn as sns\n",
    "from plotly.offline import download_plotlyjs, init_notebook_mode, plot\n",
    "import plotly.graph_objs as go\n",
    "import matplotlib.pyplot as plt\n",
    "from tg.grammar_ru.common import Loc\n",
    "from tg.grammar_ru.corpus import CorpusReader, CorpusBuilder, BucketCorpusBalancer\n",
    "from tg.grammar_ru.corpus.corpus_reader import read_data\n",
    "import os\n",
    "from pathlib import Path\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv(Loc.root_path / 'environment.env')\n",
    "from tg.grammar_ru.components.yandex_storage.s3_yandex_helpers import S3YandexHandler\n",
    "from tg.grammar_ru.components.yandex_delivery.training_logs import S3TrainingLogsLoader, TrainingLogsViewer\n",
    "\n",
    "from yo_fluq_ds import Queryable, Query, fluq\n",
    "import plotly.express as px\n",
    "from tg.grammar_ru.common import Separator\n",
    "\n",
    "from typing import List, Union\n",
    "import numpy as np\n",
    "import torch\n",
    "import math\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tg.common import DataBundle\n",
    "from tg.common.ml.batched_training import IndexedDataBundle\n",
    "from tg.grammar_ru.components.plain_context_builder import PlainContextBuilder\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "\n",
    "\n",
    "\n",
    "def get_tasks(bucket, tasks_list_s3_path):\n",
    "    tmp_local_file = Loc.temp_path / tasks_list_s3_path.split('/')[-1]\n",
    "    S3YandexHandler.download_file(bucket, tasks_list_s3_path, tmp_local_file)\n",
    "    with open(tmp_local_file, 'r') as f:\n",
    "        tasks = ast.literal_eval(f.read())\n",
    "    return tasks\n",
    "\n",
    "\n",
    "def plot_metrics(metrics, title=\"\"):\n",
    "    plt.plot(TrainingLogsViewer.get_metric_by_job(\n",
    "        metrics, 'accuracy_display'), label='accuracy_display')\n",
    "    plt.plot(TrainingLogsViewer.get_metric_by_job(\n",
    "        metrics, 'accuracy_test'), label='accuracy_test')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "def plot_loss(metrics, title=\"\"):\n",
    "    plt.plot(TrainingLogsViewer.get_metric_by_job(\n",
    "        metrics, 'loss'), label='loss')\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def plot_cm(cm):\n",
    "    fig = go.Figure(data=go.Heatmap(z=cm,\n",
    "                                    text=cm,\n",
    "                                    x=cm.columns,\n",
    "                                    y=cm.index,\n",
    "                                    texttemplate=\"%{text}\",\n",
    "                                    colorscale='Blues'))\n",
    "    fig.show()\n",
    "\n",
    "def get_label(s):\n",
    "    return int(s.split('_label_')[1])\n",
    "\n",
    "def get_true_and_pred(result_df):\n",
    "    pred_col_names = [c for c in result_df.columns if 'predicted_label' in c ]\n",
    "    true_col_names = [c for c in result_df.columns if 'true_label' in c ]\n",
    "    y_pred = result_df[pred_col_names].idxmax(axis=\"columns\").apply(get_label)\n",
    "    true_probs = result_df[true_col_names]\n",
    "    y_true = true_probs.idxmax(axis=\"columns\").apply(get_label)\n",
    "\n",
    "    result_df['pred_label'] = y_pred\n",
    "    result_df['true_label'] = y_true\n",
    "    result_df['pred_score'] = result_df[pred_col_names].max(axis=1)\n",
    "\n",
    "    return y_true, y_pred\n",
    "\n",
    "def get_worst_words_sents(result_df, src, true_label: int, pred_label: int, worst_words_cnt: int):\n",
    "    one_inst_another = result_df[(result_df.true_label == true_label) & (\n",
    "        result_df.pred_label == pred_label)]\n",
    "    thrsh = one_inst_another[f'predicted_label_{pred_label}'].sort_values(\n",
    "        ascending=False).head(worst_words_cnt).min()\n",
    "    worst_mistakes_scores = one_inst_another[\n",
    "        one_inst_another[f'predicted_label_{pred_label}'] >= thrsh]\n",
    "\n",
    "    worst_words = (src[src.word_id.isin(worst_mistakes_scores.word_id)]\n",
    "                   [['word_id', 'sentence_id', 'word']])[:worst_words_cnt]\n",
    "    worst_sents = worst_words['sentence_id'].unique()\n",
    "    worst_sents_df = src[src.sentence_id.isin(worst_sents)]\n",
    "    # worst_sents_df.loc[worst_sents_df.index, 'pred_score'] = -1\n",
    "    # worst_sents_df.loc[worst_sents_df[worst_sents_df.word_id.isin(worst_mistakes_scores.word_id)].index, \"pred_score\"] = one_inst_another.pred_score.values\n",
    "    return worst_words, worst_sents_df\n",
    "\n",
    "def get_best_words_sents(result_df, src, pred_label: int, words_cnt: int):\n",
    "    \"\"\" \n",
    "    Находит слова, в которых сеть была уверена в ответе и ответ верный\n",
    "    \"\"\"\n",
    "    correct_df = result_df[result_df.true_label==pred_label]\n",
    "    thrsh = correct_df[f'predicted_label_{pred_label}'].sort_values(\n",
    "        ascending=False).head(words_cnt).min()\n",
    "    best_scores = correct_df[correct_df[f'predicted_label_{pred_label}']>=thrsh]\n",
    "    best_words = (src[src.word_id.isin(best_scores.word_id)])[['word_id', 'sentence_id', 'word']][:words_cnt]\n",
    "    best_sents = best_words.sentence_id.unique()\n",
    "    best_sents_df = src[src.sentence_id.isin(best_sents)]\n",
    "    return best_words, best_sents_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.669212739Z",
     "start_time": "2023-10-22T12:36:01.666832033Z"
    }
   },
   "outputs": [],
   "source": [
    "project_name = 'agreementproject'\n",
    "dataset_name = 'agreement_adj_mid50_0_declination'\n",
    "bucket = 'agreementadjbucket'"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Измененный бандл:\n",
    "* Удалили slovnet\n",
    "* Удалили ОЮ"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter bundle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.711686390Z",
     "start_time": "2023-10-22T12:36:01.669365076Z"
    }
   },
   "outputs": [],
   "source": [
    "new = {'ая', 'ого', 'ое', 'ой', 'ом', 'ому',\n",
    "       'ую', 'ые', 'ый', 'ым', 'ыми', 'ых'} # тут нет окнчаний превосходных форм и ою\n",
    "\n",
    "# полнейшей, наипрочнейшего, важнейшие,меньшим, милейший, наистраннейшее, новейших, малейшем, слабейшему, меньшими\n",
    "good = {'ая', 'его', 'ее', 'ей', 'ем', 'ему',\n",
    "        'ие', 'ий', 'им', 'ими', 'их', 'ую', 'яя', 'юю'}\n",
    "\n",
    "big = {'ая', 'ие', 'им', 'ими', 'их', 'ого',\n",
    "       'ое', 'ой', 'ом', 'ому', 'ою', 'ую'}\n",
    "\n",
    "POSSIBLE_ENDINGS = set().union(new, good, big)\n",
    "endings_nums = {e: i for i, e in enumerate(\n",
    "    sorted(list(POSSIBLE_ENDINGS)))}\n",
    "num_by_ending = endings_nums\n",
    "ending_by_num = {v:k for k, v in endings_nums.items()}\n",
    "\n",
    "new_declination_labels = {num for e, num in endings_nums.items() if e in new}\n",
    "good_declination_labels = {num for e, num in endings_nums.items() if e in good}\n",
    "big_declination_labels = {num for e, num in endings_nums.items() if e in big}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.711911136Z",
     "start_time": "2023-10-22T12:36:01.711606186Z"
    }
   },
   "outputs": [],
   "source": [
    "# ! python3 -m pip install eule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.755873712Z",
     "start_time": "2023-10-22T12:36:01.711787591Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'eule'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[5], line 1\u001B[0m\n\u001B[0;32m----> 1\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01meule\u001B[39;00m\n\u001B[1;32m      2\u001B[0m diagram \u001B[38;5;241m=\u001B[39m eule\u001B[38;5;241m.\u001B[39meuler({\n\u001B[1;32m      3\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mnew\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;28mlist\u001B[39m(new),\n\u001B[1;32m      4\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mgood\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;28mlist\u001B[39m(good),\n\u001B[1;32m      5\u001B[0m     \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mbig\u001B[39m\u001B[38;5;124m'\u001B[39m:\u001B[38;5;28mlist\u001B[39m(big),\n\u001B[1;32m      6\u001B[0m                       })\n\u001B[1;32m      7\u001B[0m \u001B[38;5;28mprint\u001B[39m(diagram)\n",
      "\u001B[0;31mModuleNotFoundError\u001B[0m: No module named 'eule'"
     ]
    }
   ],
   "source": [
    "import eule\n",
    "diagram = eule.euler({\n",
    "    'new':list(new),\n",
    "    'good':list(good),\n",
    "    'big':list(big),\n",
    "                      })\n",
    "print(diagram)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В датасете только слова 1-го типа склонения. Новый. Возможно 12 окончаний. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.755683840Z"
    }
   },
   "outputs": [],
   "source": [
    "new_num_by_ending = {e:num for e,num in num_by_ending.items() if e in new}\n",
    "new_num_by_ending"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Оставили только слова типа \"Новый\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.766497352Z",
     "start_time": "2023-10-22T12:36:01.766279620Z"
    }
   },
   "outputs": [],
   "source": [
    "from tg.common import DataBundle\n",
    "from tg.common.ml.batched_training import IndexedDataBundle\n",
    "from tg.grammar_ru.components.plain_context_builder import PlainContextBuilder\n",
    "bundle_0_declination_path = Loc.data_cache_path/'bundles/agreement/mid50_0_declination'\n",
    "# bundle_full_0_declination_path = Loc.data_cache_path/'bundles/agreement/full_mystemless_0_declination'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.766437969Z"
    }
   },
   "outputs": [],
   "source": [
    "# db = DataBundle.load(Loc.data_cache_path/'bundles/agreement/mid50')\n",
    "# ids_0_type=set(db.src[db.src.declension_type==0].word_id)\n",
    "# db['index'] = db.index[db.index.word_id.isin(ids_0_type) & db.index.label.isin(new_declination_labels)]\n",
    "# db = db.copy()\n",
    "# db.save(bundle_0_declination_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.766766049Z",
     "start_time": "2023-10-22T12:36:01.766588516Z"
    }
   },
   "outputs": [],
   "source": [
    "db = DataBundle.load(Loc.data_cache_path/'bundles/agreement/mid50')\n",
    "ids_0_type=set(db.src[db.src.declension_type==0].word_id)\n",
    "db['index'] = db.index[db.index.word_id.isin(ids_0_type) & db.index.label.isin(new_declination_labels)]\n",
    "db = db.copy()\n",
    "db.save(bundle_0_declination_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Проверим отфильтрованный бандл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.766655323Z"
    }
   },
   "outputs": [],
   "source": [
    "# del db\n",
    "db = DataBundle.load(bundle_0_declination_path)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "hist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.766714126Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(db.index.label.replace(ending_by_num), histnorm=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.768981429Z",
     "start_time": "2023-10-22T12:36:01.766814056Z"
    }
   },
   "outputs": [],
   "source": [
    "db.src[db.src.word_id.isin(db.index.word_id)].declension_type.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.766884334Z"
    }
   },
   "outputs": [],
   "source": [
    "# Все возможные окончания слов 0-го типа склонения. \"Новый\"\n",
    "db.index.label.replace(ending_by_num).unique()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отправка бандла"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.767036562Z"
    }
   },
   "outputs": [],
   "source": [
    "project_name = 'agreementproject'\n",
    "# dataset_name = 'agreement_adj_mid50_0_declination'#tiny_0_declination\n",
    "dataset_name = 'agreement_adj_toy'\n",
    "\n",
    "bucket = 'agreementadjbucket'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.767094061Z"
    }
   },
   "outputs": [],
   "source": [
    "from tg.grammar_ru.components.yandex_storage.s3_yandex_helpers import S3YandexHandler\n",
    "# try:\n",
    "#     S3YandexHandler.create_bucket(bucket)\n",
    "# except:\n",
    "#     pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.767140722Z"
    }
   },
   "outputs": [],
   "source": [
    "s3path = f'datasphere/{project_name}/datasets/{dataset_name}'\n",
    "S3YandexHandler.upload_folder(bucket, s3path, bundle_0_declination_path)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Обучение\n",
    "\n",
    "Добавим в сеть кросс-энтропию."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cross-entropy & softmax 40ep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.767187447Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket, 'datasphere/agreementproject/job_info/job_agreementproject_17:01:33.277101.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                     #    normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {n,ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {n,ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.767236678Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_col_names = [c for c in result_df.columns if 'predicted_label' in c ]\n",
    "true_col_names = [c for c in result_df.columns if 'true_label' in c ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.767288478Z"
    }
   },
   "outputs": [],
   "source": [
    "# metrics[metrics.metric=='accuracy_display']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SoftmaxLess\n",
    "\n",
    "В документации сказано, что CrossEntropyLoss ожидает из сети ненормированные числа. Поэтому удалили softmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.767448868Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_18:32:21.476408.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Выводы\n",
    "\n",
    "Отлично обучилась, за исключением самого редкого класса - ОМУ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.767525704Z"
    }
   },
   "outputs": [],
   "source": [
    "db = DataBundle.load(Loc.data_cache_path/'bundles/agreement/mid50')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.768280736Z"
    }
   },
   "outputs": [],
   "source": [
    "from tg.grammar_ru.common import Separator\n",
    "\n",
    "true_label = 15\n",
    "pred_label = 20\n",
    "worst_words, worst_sents_df = get_worst_words_sents(\n",
    "    result_df, db.src, true_label=true_label, pred_label=pred_label, worst_words_cnt=4)\n",
    "print(f\"Predicted  {ending_by_num[pred_label]}  instead of  {ending_by_num[true_label]} \" )\n",
    "Separator.Viewer().tooltip(\"word_id\").color('word_id',\n",
    "                                            value_to_color={\n",
    "                                                wid: 'red' for wid in worst_words.word_id}\n",
    "                                            ).to_html_display(worst_sents_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.774895450Z",
     "start_time": "2023-10-22T12:36:01.769135544Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df.pred_score.round(6).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.769964028Z"
    }
   },
   "outputs": [],
   "source": [
    "pred_col_names = [c for c in result_df.columns if 'predicted_label' in c ]\n",
    "true_col_names = [c for c in result_df.columns if 'true_label' in c ]\n",
    "# result_df[pred_col_names].round(2)[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.770794987Z"
    }
   },
   "outputs": [],
   "source": [
    "result_df[result_df.pred_score<0.001][pred_col_names]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.771635890Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO context_size =7 \n",
    "# todo weight of class in CE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.772471701Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(result_df.true_label, result_df.pred_label))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Добавим веса классов в кросс-энтропию"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.773297301Z"
    }
   },
   "outputs": [],
   "source": [
    "def get_class_weights(db):\n",
    "    \"\"\" Normalize weights. Sum will be equal to number of classes\"\"\"\n",
    "    weights = db.index.label.value_counts().sort_index()\n",
    "    lw = torch.tensor(list(weights)).float()\n",
    "    return (lw / lw.sum()) * len(lw)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.774144301Z"
    }
   },
   "outputs": [],
   "source": [
    "bundle_0_declination_path = Loc.data_cache_path/'bundles/agreement/mid50_0_declination'\n",
    "db = DataBundle.load(bundle_0_declination_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.775390831Z",
     "start_time": "2023-10-22T12:36:01.774982880Z"
    }
   },
   "outputs": [],
   "source": [
    "len(db.src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.775855285Z"
    }
   },
   "outputs": [],
   "source": [
    "get_class_weights(db)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CE weighted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.776568307Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_07:23:35.273901.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Выводы\n",
    "\n",
    "Стало хуже.\n",
    "Возможно потому, что распределение в батче (и тем более в мини-батче) отличается от распределения во всем датасете"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context size = 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819563111Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_08:02:06.256541.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819652247Z"
    }
   },
   "outputs": [],
   "source": [
    "plot_loss(metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819677642Z"
    }
   },
   "outputs": [],
   "source": [
    "# TrainingLogsViewer.get_metric_by_job(metrics, 'loss')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819698317Z"
    }
   },
   "outputs": [],
   "source": [
    "#TODO когда менял размер контекста, заметил что assembly point создается дважды. создал 1 раз. Могут ли из-за этого быть такие артефакты"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "CE Smless twice AP CS=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819718400Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_09:13:13.607354.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819744506Z"
    }
   },
   "outputs": [],
   "source": [
    "# TrainingLogsViewer.get_metric_by_job(metrics, 'loss')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1 AP CS=7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819765336Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_08:55:47.079040.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twice AP CS=6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819789815Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_10:46:51.835344.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Вывод: примерно воспроизвели результат"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "twice AP CS=5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819809325Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_11:31:29.335750.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Context = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819829066Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_08:39:36.084751.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "Увеличение контекста не позволило избавиться от неверного предсказания \"ОМУ\".\n",
    "Влияет дисбаланс классов."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GOOD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Бандл для \"ХОРОШИЙ\" 26000 слов в индексе. 2.5 млн в src"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819849797Z"
    }
   },
   "outputs": [],
   "source": [
    "bundle_1_declination_path = Loc.data_cache_path/'bundles/agreement/mid50_1_declination'\n",
    "db = DataBundle.load(bundle_1_declination_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819871989Z"
    }
   },
   "outputs": [],
   "source": [
    "del db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819892473Z"
    }
   },
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "fig = px.histogram(db.index.label.replace(ending_by_num), histnorm=None)\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819951220Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_09:38:25.795641.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Выводы:\n",
    "аналогично \"NEW\" редкие классы не предсказываются"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GOOD stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.819981382Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_09:40:53.355391.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GOOD 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.820001667Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_10:51:38.171789.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"NEW\" Stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.820021056Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_16:24:22.924230.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.820048323Z"
    }
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# print(classification_report(y_true, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.820070618Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"BIG\" Stratified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.820090924Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_11:59:32.942012.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Вывод:\n",
    "Отлично обучилась. Надо выкинуть \"ОЮ\" и будет amazing success."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-10-22T12:36:01.825011693Z",
     "start_time": "2023-10-22T12:36:01.820111316Z"
    }
   },
   "outputs": [],
   "source": [
    "tasks = get_tasks(bucket,\n",
    " 'datasphere/agreementproject/job_info/job_agreementproject_08:51:22.641978.txt')\n",
    "\n",
    "loader = S3TrainingLogsLoader(bucket, project_name)\n",
    "metrics = loader.load_metrics(tasks)\n",
    "\n",
    "unzipped_folder = (Loc.root_path /\n",
    "                   'temp'/'training_results' /\n",
    "                   f'{tasks[0]}.unzipped')\n",
    "result_df = pd.read_parquet(unzipped_folder/'output'/'result_df.parquet')\n",
    "y_true, y_pred = get_true_and_pred(result_df)\n",
    "\n",
    "plot_metrics(metrics, tasks[0])\n",
    "sorted_nums = sorted(list(y_true.unique()))\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        # normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)\n",
    "cm = pd.DataFrame(\n",
    "    confusion_matrix(y_true, y_pred,\n",
    "                        normalize='true'\n",
    "                     ).round(2),\n",
    "    columns=[f'pred {ending_by_num[n]}({n})' for n in sorted_nums],\n",
    "    index=[f'actual {ending_by_num[n]}({n})' for n in sorted_nums]\n",
    ")\n",
    "plot_cm(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2023-10-22T12:36:01.820131552Z"
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
